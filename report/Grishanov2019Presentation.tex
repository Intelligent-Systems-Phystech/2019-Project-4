	\documentclass{beamer}
\usepackage[cp1251]{inputenc}        % Russian language coding
\usepackage[russian]{babel}  % Russian layout
\usepackage{amsmath,mathrsfs,amsfonts,amssymb}
\usepackage{graphicx, epsfig}
\usepackage{subfig}
\usepackage{floatflt}
\usepackage{epic,ecltree}
\usepackage{mathtext}
\usepackage{fancybox}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{epstopdf}

\graphicspath{../Grishanov2019Project4/img}

%\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
%\newcommand\argmin{\mathop{\arg\min}}

\usetheme{Warsaw}%{Singapore}%{Warsaw}%{Darmstadt}
\usecolortheme{sidebartab}

\title[\hbox to 56mm{Сравнение тематических моделей  \hfill\insertframenumber\,/\,\inserttotalframenumber}]
            {Автоматическая настройка параметров BigARTM\\
             под широкий класс задач}
\author{Гришанов А.\,В.}
\institute[МФТИ]{Московский физико-технический институт \\
    Факультет управления и прикладной математики\\
    Кафедра <<Интеллектуальные системы>>
    \vspace{0.3cm} \\
    Задачу поставил к.ф.-м.н., н.с. ВЦ РАН К.\,В.\,Воронцов \\
		Консультант Мурат Апишев
}

\date{
    Москва,\\
    2019\,г.
}

\begin{document}


% Creates title page of slide show using above information
\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Цель работы}

\begin{block}{Проблема}
BigARTM --- продвинутая бибиотека для тематического моделирования. В ней реализовано много регуляризаторов, что повышает гибкость, но при этом усложняет настройку параметров. В результате на практике популярнее более простые методы, такие как LDA.
\end{block}

\begin{block}{Цель работы}
Проверить гипотезу о существовании конфигураций, хорошо работающих на широком классе задач.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
	\frametitle{Базовые алгоритмы}
	
\begin{enumerate}

\item
Логистическая регрессия с $L_1$ регуляризацией

\item
PLSA

David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
Learning research, 3(Jan):993–1022, 2003.
\item
LDA
Thomas Hofmann. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth Conference
on Uncertainty in Artificial Intelligence, UAI’99, pages 289–296, San Francisco, CA, USA, 1999.
Morgan Kaufmann Publishers Inc.
\end{enumerate}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Стартовые обозначения}
\begin{itemize}
\item $D$ --- коллекция текстовых документов, состоящая из документов $d$ \item $W$ --- словарь, состоящий из терминов $w$; 
\item $T$~--- множество тем, состоящее из тем $t$.
\end{itemize}

Согласно формуле полной вероятности и гипотезе условной независимости, распределение термов
в документе $p(w|d)$ описывается вероятностной смесью распределений термов в темах $\varphi_{wt} = p(w|t)$ с весами $\theta_{td} = p(t|d)$

\begin{block}{Распределение термов в документах}
	\begin{equation}\label{eq_0}
	p(w|d) = \sum\limits_{t \in T} p(w|t) p(t|d) = \sum\limits_{t \in T}\phi_{wt}\theta_{td}.
	\end{equation}
\end{block}

%Вероятностная тематическая модель (ВТМ) описывает каждую тему дискретным распределением на множестве терминов $\phi_{wt} = p(w|t)$, каждый документ ---  дискретным распределением на множестве тем $\theta_{td} = p(t|d)$. Предполагается, что коллекция документов --- это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке.
\end{frame}


\begin{frame}
	
$$\includegraphics[width=1.1\linewidth]{../Grishanov2019Project4/img/generation}$$
	
\begin{block}{Распределение термов в документах}
\begin{equation}\label{eq_0}
p(w|d) = \sum\limits_{t \in T} p(w|t) p(t|d) = \sum\limits_{t \in T}\phi_{wt}\theta_{td}.
\end{equation}
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{frame}
  \frametitle{Постановка задачи тематического моделирования}

Ставится задача разложения матрицы $F$ в произведение двух матриц $\Phi$ и $\Theta$ меньшего размера

$$\includegraphics[scale=0.3]{decomposition.jpg}$$

%\begin{itemize}
%\item $\Phi = (\phi_{wt})_{W \times T}, \; \phi_{wt} = p(w|t)$ --- матрица <<термины-темы>>;
%\item $\Theta = (\theta_{td})_{T \times D}, \; \theta_{td} = p(t|d)$ --- матрица <<темы-документы>>;
%\end{itemize}


Поставленная задача ($F \approx \Phi \Theta$) эквивалентна поиску матриц $\Phi$ и $\Theta$, максимизирующих следующий функционал:
\begin{block}{Задача}
\begin{equation}\label{eq_1}
 	L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \sum_{t \in T} \phi_{wt} \theta_{td} \rightarrow \max_{\Phi, \Theta}
\end{equation}
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Сложности, возникающие при решении задачи}

\begin{block}{Проблема}
Разложение матрицы $F$ в произведение матриц $\Phi$ и $\Theta$ не единственно. В частности, для любой невырожденной матрицы $S$ размера $TxT$ верно, что $F = (\Phi S)(S^{-1} \Theta)$.
\end{block}

Таким образом, из-за сложившейся неопределённости, невозможен поиск произвольного матричного разложения, нужно уточнять модель. Можно наложить дополнительные ограничения, что приведёт к сокращению произвольности выбора или же сделать некоторые предположения о вероятностном распределении коллекции. Рассматриваются следующие два подхода к решению проблемы:

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Подходы к устранению неопределённости}

\begin{block}{Латентное размещение Дирихле}
Тематическая модель латентного размещения Дирихле (latent Dirichlet
allocation, LDA) основана на дополнительном предположении, что векторы документов $\theta_{d} = (\theta_{td}) \in \mathbb{R}^{|T|}$ и векторы тем $\phi_{t} = (\phi_{wt}) \in \mathbb{R}^{|W|}$ порождаются распределениями Дирихле с параметрами $\alpha \in \mathbb{R}^{|T|}$ и $\beta \in \mathbb{R}^{|W|}$.
\end{block}

\begin{block}{Аддитивная регуляризация}
Тематическая модель аддитивной регуляризации (additive regularization of topic models, ARTM) получается при наложении на модель дополнительных требований (регуляризаторов).
\begin{equation}
	L(\Phi, \Theta) + \sum_{i=1}^{n} \tau_i R_i(\Phi, \Theta) \rightarrow \max_{\Phi, \Theta}
\end{equation}	
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%    \frametitle{Библиотека тематического моделирования BigARTM }
%BigARTM ("`Big"' --- от <<большие данные>>) - открытая кросс-платформенная распределённая библиотека тематического моделирования, основанная на идее аддитивной регуляризации. \textit{(Для более подробной информации стоит зайти на \href{http://bigartm.org}{\textbf{bigartm.org}})}.
%\begin{itemize}
%\item BigARTM не загружает текстовую коллекцию полностью в память, а разбивает её на фрагменты и всякий раз работает только с определённым числом из них.
%\item Параллельно может обрабатываться несколько фрагментов коллекции. Также BigARTM даёт возможность распределить вычислительные процедуры по целому кластеру компьютеров.
%\item BigARTM является Open source проектом, поэтому можно легко модифицировать исходный код и приспосабливать его под свои эксперименты.
%\end{itemize}
%
%\end{frame}


\begin{frame}
\frametitle{Решение}
\begin{itemize}
\item
Рассмотрим набор датасетов $\left\{\mathfrak{D}_{ex}, \mathfrak{D}_{in}\right\}$, где  $\mathfrak{D}_{ex}$ имеют внешний критерий качества, а $\mathfrak{D}_{in}$~ --- только внутренние.

\item
Необходимо проверить гипотезу о том, что существуют коэффициенты регуляризации $\tau_{general}$, которые можно считать <<универсальными>>, т.е. для которых метрики качества отличаются от лучших на том же датасете не более чем на 5\%.

\item
Для каждого из первых найдём лучшие параметры, затем будем искать общие.

\item В конце проверим выполнение гипотезы на всех данных, для неразмеченных будем сравнивать внутренние критерии качества.

\end{itemize}

Цель работы --- построить модель, которая {\bf не хуже} чем PLSA и {\bf лучше} PLSA по нескольким критериям.

\end{frame}


\begin{frame}
	\frametitle{Результаты эксперимента}

\begin{enumerate}
\item {\bf 20news groups}

Best f1\_score: 0.9155

General params f1\_score: 0.9148

\item {\bf Victorian Era}

Best f1\_score: 0.9777

General params f1\_score: 0.9777

\item {\bf Toxic comments}

Best f1\_score: 0.9539

General params f1\_score: 0.9582

\end{enumerate}	
	
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{frame}
%     \frametitle{Сравнение библиотек}
%Для сравнения тематических моделей предлагается рассмотреть следующие функционалы качества:
%\begin{itemize}
%\item Перплексия (perplexity) 
%\begin{equation}
%\textbf{P}(D;p)= \exp \left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw} \ln p(w|d) \right)
%\end{equation}
%\item Когерентность (coherence)
%\begin{equation}
%\textbf{LCP}(t)= \sum_{i=1}^{k-1} \sum_{j=i}^{k} \log \frac{N(w_i,w_j)}{N(w_i)}
%\end{equation}
%\item Разреженность (sparsity)
%\begin{equation} 
%\textbf{Sp}_1(w,t)= \frac{[\phi_{wt} = 0]}{[\phi_{wt}]} \times {100 \% } ,~~~ \textbf{Sp}_2(t,d)= \frac{[\theta_{td} = 0]}{[\theta_{td}]} \times {100 \% }
%\end{equation}
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}
%	\frametitle{Ядерные метрики качества}
%\begin{block}{Ядро темы}
%Будем относить слово $w$ к ядру темы $t$, если $p(t|w) > \delta$, где $\delta$ - параметр, который задаётся в эксперименте. Обозначим ядро темы $t$ через $W_t$ и определим три показателя интерпретируемости темы.
%\end{block}
%\begin{itemize}
%\item Размер ядер тем (Topic kernel size)
%\begin{equation}
%\textbf{Ker}(t) = |{W_t}|
%\end {equation}
%\item Чистота ядер тем (Topic kernel purity)
%\begin{equation}
%\textbf{Pur}(t) = \sum_{w \in W_t} p(w|t)
%\end {equation}
%\item Контрастность ядер тем (Topic kernel contrast)
%\begin{equation}
%\textbf{Con}(t) = \frac{1}{|{W_t}|} \sum_{w \in W_t} p(t|w)
%\end {equation}
%\end{itemize}
%
%\end{frame}
%

\end{document}