\documentclass{beamer}
\usepackage[cp1251]{inputenc}        % Russian language coding
\usepackage[russian]{babel}  % Russian layout
\usepackage{amsmath,mathrsfs,amsfonts,amssymb}
\usepackage{graphicx, epsfig}
\usepackage{subfig}
\usepackage{floatflt}
\usepackage{epic,ecltree}
\usepackage{mathtext}
\usepackage{fancybox}
\usepackage{fancyhdr}
\usepackage{enumerate}
\usepackage{epstopdf}

\graphicspath{../Grishanov2019Project4/img}

%\newcommand{\T}{^{\text{\tiny\sffamily\upshape\mdseries T}}}
%\newcommand\argmin{\mathop{\arg\min}}

\usetheme{Warsaw}%{Singapore}%{Warsaw}%{Darmstadt}
\usecolortheme{sidebartab}

\title[\hbox to 56mm{Сравнение тематических моделей  \hfill\insertframenumber\,/\,\inserttotalframenumber}]
            {Автоматическая настройка параметров BigARTM\\
             под широкий класс задач}
\author{Гришанов А.\,В.}
\institute[МФТИ]{Московский физико-технический институт \\
    Факультет управления и прикладной математики\\
    Кафедра <<Интеллектуальные системы>>
    \vspace{0.3cm} \\
    Задачу поставил д.ф.-м.н., К.\,В.\,Воронцов \\
		Консультант Виктор Булатов
}

\date{
    Москва,\\
    2019\,г.
}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Цель работы}

\begin{block}{Проблема}

Настройка параметров BigARTM требует работы аналитика (эксперта). Требуется автоматизировать этот процесс.

\end{block}

\begin{block}{Цель работы}
Проверить гипотезу о существовании конфигураций, хорошо работающих на широком классе задач.
\end{block}

\begin{block}{Метод решения}
	Предлагается использовать относительные коэффициенты регуляризации и автоматический подбор $n$-граммм.
\end{block}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
	\frametitle{Относительные коэффициенты регуляризации}

%Множество всех распределений над словарем $W$ образует симплекс тем в $|W|$-мерном пространстве:

%Каждая точка этого симплекса соответствует некоторому распределению.

%Регуляризатор сглаживания на М-шаге смещает оценку максимального правдоподобия $\frac{n_{wt}}{n_t}$
%в сторону априорного распределения $\beta_w$ на вектор, длина которого пропорциональна расстоянию между распределениями, с коэффициентом пропорциональности $\lambda$.
%
%Регуляризатор разреживания, наоборот, отдаляет оценку максимального правдоподобия от заданного априорного распределения.
%В случае, когда вектор сдвига слишком большой, оценка остается на границе симплекса благодаря операции положительной срезки, при этом некоторые компоненты распределения зануляются.

\begin{figure}[h]
	\subfloat[Сглаживание]{\includegraphics[scale=0.6]{img/rel_smooth.png}}
	\subfloat[Разреживание]{\includegraphics[scale=0.6]{img/rel_sparse.png}}\\
	%	\caption{Подпись должна размещаться под рисунком. }
	\label{fg:Example}
\end{figure}

\end{frame}

%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}
%	\frametitle{Базовые алгоритмы}
%	
%\begin{enumerate}
%
%\item
%Логистическая регрессия с $L_2$ регуляризацией
%
%\item
%PLSA
%
%David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. Journal of machine
%Learning research, 2003.
%\item
%LDA
%Thomas Hofmann. Probabilistic latent semantic analysis. In Proceedings of the Fifteenth Conference
%on Uncertainty in Artificial Intelligence, UAI’99, pages 289–296, San Francisco, CA, USA, 1999.
%\end{enumerate}
%
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Обозначения}
\begin{itemize}
\item $d \in D$ --- текстовые документы
\item $w\in W$ --- слова
\item $t\in T$~--- темы
\end{itemize}

%Согласно формуле полной вероятности и гипотезе условной независимости, распределение термов
%в документе $p(w|d)$ описывается вероятностной смесью распределений термов в темах $\varphi_{wt} = p(w|t)$ с весами $\theta_{td} = p(t|d)$

\begin{block}{Распределение термов в документах}
	\begin{equation}\label{eq_0}
	p(w|d) = \sum\limits_{t \in T} p(w|t) p(t|d) = \sum\limits_{t \in T}\varphi_{wt}\theta_{td}.
	\end{equation}
\end{block}


%Вероятностная тематическая модель (ВТМ) описывает каждую тему дискретным распределением на множестве терминов $\phi_{wt} = p(w|t)$, каждый документ ---  дискретным распределением на множестве тем $\theta_{td} = p(t|d)$. Предполагается, что коллекция документов --- это последовательность терминов, выбранных случайно и независимо из смеси таких распределений, и ставится задача восстановления компонент смеси по выборке.
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5

\begin{frame}
  \frametitle{Задача тематического моделирования}

Ставится задача разложения матрицы $F$ в произведение двух матриц $\Phi$ и $\Theta$ меньшего размера

$$\includegraphics[scale=0.3]{img/phi_theta.png}$$

%\begin{itemize}
%\item $\Phi = (\phi_{wt})_{W \times T}, \; \phi_{wt} = p(w|t)$ --- матрица <<термины-темы>>;
%\item $\Theta = (\theta_{td})_{T \times D}, \; \theta_{td} = p(t|d)$ --- матрица <<темы-документы>>;
%\end{itemize}


Поставленная задача ($F \approx \Phi \Theta$) эквивалентна поиску матриц $\Phi$ и $\Theta$, максимизирующих следующий функционал:
\begin{block}{Задача}
\begin{equation}\label{eq_1}
 	L(\Phi, \Theta) = \sum_{d \in D} \sum_{w \in d} n_{dw} \sum_{t \in T} \phi_{wt} \theta_{td} \rightarrow \max_{\Phi, \Theta}
\end{equation}
\end{block}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
    \frametitle{Сложности, возникающие при решении задачи}

\begin{block}{Проблема}
Разложение матрицы $F$ в произведение матриц $\Phi$ и $\Theta$ не единственно. В частности, для любой невырожденной матрицы $S$ размера $TxT$ верно, что $F = (\Phi S)(S^{-1} \Theta)$.
\end{block}

\begin{block}{Аддитивная регуляризация}
	Тематическая модель аддитивной регуляризации (additive regularization of topic models, ARTM) получается при наложении на модель дополнительных требований (регуляризаторов).
	\begin{equation}
	L(\Phi, \Theta) + \sum_{i=1}^{n} \tau_i R_i(\Phi, \Theta) \rightarrow \max_{\Phi, \Theta}
	\end{equation}	
\end{block}



%Таким образом, из-за сложившейся неопределённости, невозможен поиск произвольного матричного разложения, нужно уточнять модель. Можно наложить дополнительные ограничения, что приведёт к сокращению произвольности выбора или же сделать некоторые предположения о вероятностном распределении коллекции. Рассматриваются следующие два подхода к решению проблемы:

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%    \frametitle{Подходы к устранению неопределённости}
%
%\begin{block}{Аддитивная регуляризация}
%Тематическая модель аддитивной регуляризации (additive regularization of topic models, ARTM) получается при наложении на модель дополнительных требований (регуляризаторов).
%\begin{equation}
%	L(\Phi, \Theta) + \sum_{i=1}^{n} \tau_i R_i(\Phi, \Theta) \rightarrow \max_{\Phi, \Theta}
%\end{equation}	
%\end{block}
%
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{frame}
%    \frametitle{Библиотека тематического моделирования BigARTM }
%BigARTM ("`Big"' --- от <<большие данные>>) - открытая кросс-платформенная распределённая библиотека тематического моделирования, основанная на идее аддитивной регуляризации. \textit{(Для более подробной информации стоит зайти на \href{http://bigartm.org}{\textbf{bigartm.org}})}.
%\begin{itemize}
%\item BigARTM не загружает текстовую коллекцию полностью в память, а разбивает её на фрагменты и всякий раз работает только с определённым числом из них.
%\item Параллельно может обрабатываться несколько фрагментов коллекции. Также BigARTM даёт возможность распределить вычислительные процедуры по целому кластеру компьютеров.
%\item BigARTM является Open source проектом, поэтому можно легко модифицировать исходный код и приспосабливать его под свои эксперименты.
%\end{itemize}
%
%\end{frame}


\begin{frame}
\frametitle{Решение}
\begin{itemize}
\item
Рассмотрим набор датасетов $\left\{\mathfrak{D}_{ex}, \mathfrak{D}_{in}\right\}$, где  $\mathfrak{D}_{ex}$ имеют внешний критерий качества, а $\mathfrak{D}_{in}$~ --- только внутренние.

\item
Необходимо проверить гипотезу о том, что существуют общие коэффициенты регуляризации $\tau_{general}$, для которых метрики качества отличаются от лучших на том же датасете не более чем на 5\%.

\item
Для каждого из первых найдём лучшие параметры, затем будем искать общие.

\item В конце проверим выполнение гипотезы на всех данных.

\end{itemize}

Критерий --- построить модель, которая {\bf не хуже} чем PLSA и {\bf лучше} PLSA по нескольким критериям.

\end{frame}

\begin{frame}
	\frametitle{Результаты эксперимента}

\begin{enumerate}
\item {\bf 20news groups}

Best f1\_score: 0.9155

General params f1\_score: 0.9148

\item {\bf Victorian Era}

Best f1\_score: 0.9777

General params f1\_score: 0.9777

\item {\bf Toxic comments}

Best f1\_score: 0.9539

General params f1\_score: 0.9582

\end{enumerate}	
	
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\begin{frame}
%     \frametitle{Сравнение библиотек}
%Для сравнения тематических моделей предлагается рассмотреть следующие функционалы качества:
%\begin{itemize}
%\item Перплексия (perplexity) 
%\begin{equation}
%\textbf{P}(D;p)= \exp \left(- \frac{1}{n} \sum_{d \in D} \sum_{w \in d} n_{dw} \ln p(w|d) \right)
%\end{equation}
%\item Когерентность (coherence)
%\begin{equation}
%\textbf{LCP}(t)= \sum_{i=1}^{k-1} \sum_{j=i}^{k} \log \frac{N(w_i,w_j)}{N(w_i)}
%\end{equation}
%\item Разреженность (sparsity)
%\begin{equation} 
%\textbf{Sp}_1(w,t)= \frac{[\phi_{wt} = 0]}{[\phi_{wt}]} \times {100 \% } ,~~~ \textbf{Sp}_2(t,d)= \frac{[\theta_{td} = 0]}{[\theta_{td}]} \times {100 \% }
%\end{equation}
%\end{itemize}
%
%\end{frame}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%\begin{frame}
%	\frametitle{Ядерные метрики качества}
%\begin{block}{Ядро темы}
%Будем относить слово $w$ к ядру темы $t$, если $p(t|w) > \delta$, где $\delta$ - параметр, который задаётся в эксперименте. Обозначим ядро темы $t$ через $W_t$ и определим три показателя интерпретируемости темы.
%\end{block}
%\begin{itemize}
%\item Размер ядер тем (Topic kernel size)
%\begin{equation}
%\textbf{Ker}(t) = |{W_t}|
%\end {equation}
%\item Чистота ядер тем (Topic kernel purity)
%\begin{equation}
%\textbf{Pur}(t) = \sum_{w \in W_t} p(w|t)
%\end {equation}
%\item Контрастность ядер тем (Topic kernel contrast)
%\begin{equation}
%\textbf{Con}(t) = \frac{1}{|{W_t}|} \sum_{w \in W_t} p(t|w)
%\end {equation}
%\end{itemize}
%
%\end{frame}
%

\end{document}